# -*- coding: utf-8 -*-
"""HW2.2_DL*.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U2xLS9aNDtOjFFK86wwM8DlLbpGtPy-i
"""

import numpy as np 
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
np.random.seed(1)
data = datasets.load_iris()
X=data.data
y=data.target.reshape(-1, 1)
encoder = OneHotEncoder(sparse=False)
y = encoder.fit_transform(y)
train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.2)
class Neural_Network(object):
  def __init__(self):
    self.input=4 
    self.hidden1=4
    self.hidden2=4
    self.hidden3=4
    self.output=3
    self.W1=np.random.normal(0,1,size=(self.input,self.hidden1)) 
    self.W2=np.random.normal(0,1,size=(self.hidden1,self.hidden2))
    self.W3=np.random.normal(0,1,size=(self.hidden2,self.hidden3))
    self.W4=np.random.normal(0,1,size=(self.hidden3,self.output)) 

  def forward(self,X):
    #forward propagation
    self.a1=np.dot(X,self.W1) 
    self.h1=self.sigmoid(self.a1) 
    self.a2=np.dot(self.h1,self.W2)
    self.h2=self.sigmoid(self.a2)
    self.a3=np.dot(self.h2,self.W3)
    self.h3=self.sigmoid(self.a3)
    self.a4=np.dot(self.h3,self.W4)
    y_hat = self.softmax(self.a4) 
    return y_hat 

  def sigmoid(self, s):
    return 1/(1+np.exp(-s))

  def derivative_sigmoid(self,x):
        return x * (1 - x)
  def softmax(self,x):
    shiftx = x - np.max(x)
    exps = np.exp(shiftx)
    return exps / np.sum(exps)     
  def derivative_softmax(self,x,z):
    for i in range(0,len(x)):
      for j in range(0,len(z)):
        if i==j:
          return z[i]*(1-z[i])
        else:
          return -z[i]*z[j]  
  
  def backpro(self,X,y,y_hat):
    self.y_hat_err=y-(y_hat)
    self.y_hat_d=np.dot(self.y_hat_err,self.derivative_softmax(self.a4,y_hat)) 
    self.W4_d=np.dot(self.h3.reshape(4,1),self.y_hat_d.reshape(1,3))
    self.h3_d=np.dot(self.W4,self.y_hat_d)
    self.a3_d=np.dot(self.h3_d,self.derivative_sigmoid(self.a3))
    self.W3_d=np.dot(self.a3_d,self.h2.T)
    self.h2_d=np.dot(self.W3.T,self.a3_d)
    self.a2_d=np.dot(self.h2_d,self.derivative_sigmoid(self.a2))
    self.W2_d=np.dot(self.a2_d,self.h1.T)
    self.h1_d=np.dot(self.W2.T,self.a2_d)
    self.a1_d=np.dot(self.h1_d,self.derivative_sigmoid(self.a1))
    self.W1_d=np.dot(self.a1_d,X.T)
    self.W1+=np.dot(self.W1_d.T,X)
    self.W2+=np.dot(self.W2_d.T,self.h1)
    self.W3+=np.dot(self.W3_d.T,self.h2)    
    self.W4+=np.dot(self.W4_d.T,self.h3)
  def train(self, X, y):
    o = self.forward(X)
    self.backpro(X, y, o)
#################    
NN = Neural_Network()
s1=0
for i in range(0,len(train_x)):
  s1+=np.square(np.argmax(train_y[i]) - np.argmax(NN.forward(train_x[i])))
  NN.train(train_x[i],train_y[i])
print("MSE Loss for train data:",((1/len(train_x))*s1))
s2=0  
for j in range(0,len(test_x)):
  s2+=np.square(np.argmax(test_y[j]) - np.argmax(NN.forward(test_x[j])))
print("MSE Loss for test data:",((1/len(test_x))*s2))

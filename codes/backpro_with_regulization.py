# -*- coding: utf-8 -*-
"""HW4.9_DL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZV8dAdxn5-H3zZKpCg9Q-MzwA0E2DA2z
"""

import numpy as np 
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
import math
np.random.seed(1)
data = datasets.load_iris()
X=data.data
y=data.target.reshape(-1, 1)
encoder = OneHotEncoder(sparse=False)
y = encoder.fit_transform(y)
train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.2)
class Neural_Network(object):
  def __init__(self):
    self.input=4 
    self.hidden1=2
    self.output=3
    self.W1=np.random.normal(0,1,size=(self.input,self.hidden1)) 
    self.W2=np.random.normal(0,1,size=(self.hidden1,self.output)) 
  def forward(self,X):
    #forward propagation
    self.a1=np.dot(X,self.W1) 
    self.h1=self.sigmoid(self.a1) 
    self.a2=np.dot(self.h1,self.W2)
    y_hat = self.softmax(self.a2) 
    return y_hat 

  def sigmoid(self, s):
    return 1/(1+np.exp(-s))

  def derivative_sigmoid(self,x):
        return x * (1 - x)
  def softmax(self,x):
    shiftx = x - np.max(x)
    exps = np.exp(shiftx)
    return exps / np.sum(exps)     
  def derivative_softmax(self,x,z):
    for i in range(0,len(x)):
      for j in range(0,len(z)):
        if i==j:
          return z[i]*(1-z[i])
        else:
          return -z[i]*z[j]  
  ################### ta inja 
  def backpro(self,X,y,y_hat):
    self.y_hat_err=y-(y_hat)
    self.y_hat_d=np.dot(self.y_hat_err,self.derivative_softmax(self.a2,y_hat))
    self.W2_d=np.dot(self.h1.reshape(2,1),self.y_hat_d.reshape(1,3))+self.W2
    self.h1_d=np.dot(self.W2,self.y_hat_d)
    self.a1_d=np.multiply(self.h1_d,self.derivative_sigmoid(self.a1))
    self.W1_d=np.dot(X.reshape(4,1),self.a1_d.reshape(1,2))+self.W1
    self.W1+=np.dot(self.W1_d.T,X)
    self.W2+=np.dot(self.W2_d.T,self.h1)
  def train(self, X, y):
    o = self.forward(X)
    self.backpro(X, y, o)  
#################    
s=0
NN = Neural_Network()
for i in range(0,len(train_x)):
  NN.train(train_x[i],train_y[i])
  if math.isnan(NN.W1[0][0])==False:
    if  abs(NN.W1[0][0])<100:
      s+=np.linalg.norm(NN.W1)
      s+=np.linalg.norm(NN.W2)   
s1=0
for i in range(0,len(train_x)):
  s1+=np.square(np.argmax(train_y[i]) - np.argmax(NN.forward(train_x[i])))    
print("MSE Loss for train data:",((1/len(train_x))*(s1+s)))
s2=0  
for j in range(0,len(test_x)):
  s2+=np.square(np.argmax(test_y[j]) - np.argmax(NN.forward(test_x[j])))
print("MSE Loss for test data:",((1/len(test_x))*(s2+s)))
